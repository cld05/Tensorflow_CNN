import matplotlib.pyplot as plt
from PIL import Image
import tensorflow as tf
import numpy as np
import time
from datetime import timedelta
import math
import os
import random

# Definition of the properties of the CNN layers
# Layer 1
filter_size1 = 7  # filter 5x5 pixels
num_filters1 = 16  # number of filters in the first layer

# Layer 2
filter_size2 = 7  # filter size 5x5 pixels
num_filters2 = 36  # number of filters in the second layer

# Final fully-connected layer
fc_size = 64

# Define the size of the stride operation
stride = 1

# Data download from MNIST dataset
from tensorflow.examples.tutorials.mnist import input_data
data = input_data.read_data_sets('data/MNIST/', one_hot=True)

img_index_train = [i for i in range(len(data.train.images))]
img_index_train_rndm = random.sample(range(len(img_index_train)), int(len(img_index_train) / 2))
# print(data.train.images[img_index_rndm[1]])
for k in img_index_train_rndm:
    data.train.images[k] = 1 - data.train.images[k]

img_index_test = [i for i in range(len(data.test.images))]
img_index_test_rndm = random.sample(range(len(img_index_test)), int(len(img_index_test) / 2))
for k in img_index_test_rndm:
    data.test.images[k] = 1 - data.test.images[k]


# The dataset above gives one hot encoded labels
# Here we create labels associated to class name
data.test.cls = np.argmax(data.test.labels, axis=1)
data.validation.cls = np.argmax(data.validation.labels, axis=1)

# Definition of the size of our data
img_size = 28  # images are 28x28 pixles (1 channel since balck and white)
img_flat_size = img_size * img_size  # Size of flattened image
# tuple to be used when reshaping flattened image in 2-dimensional array
img_shape = (img_size, img_size)
num_channels = 1  # black and white images have only 1 channel
num_cls = 10  # we will calssify handwritten digits from 0 to 9


# Helper functions

def new_weights(shape):  # used to retrieve the weights of the different layers
    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))


def new_biases(length):  # used to retrieve the biases of the different layers
    return tf.Variable(tf.constant(0.05, shape=[length]))

# This funciton is used to create new convolutional layer
# Each convolutional layer must take into account the output of the previous one


def leaky_relu(x, alpha):       # We will use a leaky_relu activatin function
    return tf.nn.relu(x) - alpha * tf.nn.relu(-x)


def new_conv_layer(input, num_input_channels, filter_size, num_filters, use_pooling=True):

    # Shape of the filter-weights
    # This shape is coherent with what imposed by tensorflow API
    shape = [filter_size, filter_size, num_input_channels, num_filters]

    # The weights for the filter are created here using the function
    # defined above and the variable 'shape' just defined
    weights = new_weights(shape=shape)

    # The biases for the filters are created using the helper funciton
    # created above and the variable 'num_filters'
    biases = new_biases(length=num_filters)

    # We can now create the convolutional layer using the variables defined above
    layer = tf.nn.conv2d(input=input,
                         filter=weights,
                         strides=[1, stride, stride, 1],
                         padding='SAME')
    # Adding the biases to the filters of the layer
    layer += biases

    # if we have chose to use pooling we apply it
    if use_pooling:
        layer = tf.nn.max_pool(value=layer,
                               ksize=[1, 2, 2, 1],
                               strides=[1, stride, stride, 1],
                               padding='SAME')
    layer = leaky_relu(layer, alpha=0.2)

    return layer, weights


# The output of the convolutional layers is 4-dimensional
# we need to reduce it to a 2-dimensional one


def flatten_layer(layer):
    layer_shape = layer.get_shape()
    num_features = layer_shape[1:4].num_elements()
    layer_flat = tf.reshape(layer, [-1, num_features])

    return layer_flat, num_features


def new_fc_layer(input,
                 num_inputs,
                 num_outputs,
                 use_relu=True):
    weights = new_weights(shape=[num_inputs, num_outputs])
    biases = new_biases(length=num_outputs)

    layer = tf.matmul(input, weights) + biases

    if use_relu:
        layer = leaky_relu(layer, alpha=0.2)

    return layer


def optimize(num_iterations):

    global total_iterations  # update global variable
    global best_validation_accuracy
    global last_improvement

    start_time = time.time()  # initial time, stored for priting purposes

    for i in range(num_iterations):

        # Update total number of iterations
        total_iterations += 1

        # Get a batch of the training data and corresponsing labels
        x_batch, y_true_batch = data.train.next_batch(train_batch_size)

        # Create a feed dictionary
        feed_dict_train = {x: x_batch,
                           y_true: y_true_batch}

        # run the optimizier in the sessionself
        # tensorflow uses the dictionarycreated above in order to feed
        # the data into place data into the placeholders
        session.run(optimizer, feed_dict=feed_dict_train)

        # Print status messages
        if total_iterations % 100 == 0:
            # calculate accuracy on training batch
            acc_train = session.run(accuracy, feed_dict=feed_dict_train)

            acc_validation, _ = validation_accuracy()

            if acc_validation > best_validation_accuracy:
                best_validation_accuracy = acc_validation

                last_improvement = total_iterations

                saver.save(sess=session, save_path=save_path)

                improved_string = '*'

            else:
                improved_string = ' '

            msg = "Optimization Iteration: {0:>6}, Train batch accuracy {1:>6.1%}, Validation Acc:{2:>6.1%}{3}"

            print(msg.format(i + 1, acc_train, acc_validation, improved_string))

        if total_iterations - last_improvement > required_improvement:
            print("No improvement found, stopping optimization")
            break

    # End time
    end_time = time.time()

    # Total time
    time_diff = end_time - start_time

    print("Time usage:" + str(timedelta(seconds=int(round(time_diff)))))


def predict_cls(images, labels, cls_true):
    batch_size = 256
    num_images = len(images)

    cls_pred = np.zeros(shape=num_images, dtype=np.int)

    i = 0
    while i < num_images:
        # The ending index for the next batch is denoted j.
        j = min(i + batch_size, num_images)

        # Create a feed-dict with the images and labels
        # between index i and j.
        feed_dict = {x: images[i:j, :],
                     y_true: labels[i:j, :]}

        # Calculate the predicted class using TensorFlow.
        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)

        # Set the start-index for the next batch to the
        # end-index of the current batch.
        i = j

    correct = (cls_true == cls_pred)

    return correct, cls_pred


def cls_accuracy(correct):
    print(correct)
    correct_sum = correct.sum()

    acc = float(correct_sum) / len(correct)

    return acc, correct_sum


def predict_cls_validation():
    return predict_cls(images=data.validation.images,
                       labels=data.validation.labels,
                       cls_true=data.validation.cls)


def predict_cls_test():
    return predict_cls(images=data.test.images,
                       labels=data.test.labels,
                       cls_true=data.test.cls)


def validation_accuracy():
    correct, _ = predict_cls_validation()
    return cls_accuracy(correct)


def plot_image(image, save=False, name='test.png'):
    fig = plt.figure()
    plt.imshow(image.reshape(28, 28),
               interpolation='nearest',
               cmap='binary')

    plt.show()
    if save:
        path = os.path.join(
            '/Users/claudiopaliotta/Documents/MachineLearning/Tensorflow_CNN/', name)
        fig.savefig(path)


def plot_image_pred(image, label, save=False, name='test_lbl.png'):
    fig = plt.figure()
    plt.imshow(image.reshape(28, 28),
               interpolation='nearest',
               cmap='binary')
    plt.title(label)
    plt.show()
    if save:
        path = os.path.join(
            '/Users/claudiopaliotta/Documents/MachineLearning/Tensorflow_CNN/', name)
        fig.savefig(path)


def print_test_accuracy():

    correct, cls_pred = predict_cls_test()
    acc, num_correct = cls_accuracy(correct=correct)
    msg = "Accuracy on Test-set:{0:.1%} ({1}/{2})"
    num_images = len(correct)
    print(msg.format(acc, num_correct, num_images))


#
# -------------------------------------------------------#
#
# Definition of the variables and the model
# Create place holder variables
# variable for the fully connected layer
x = tf.placeholder(tf.float32, shape=[None, img_flat_size], name='x')
# variable for the convolutional layers
x_img = tf.reshape(x, [-1, img_size, img_size, num_channels])
# variable for true values of the MNIST data
y_true = tf.placeholder(tf.float32, shape=[None, 10], name='y_true')
y_true_cls = tf.argmax(y_true, dimension=1)

# We can now create the convolutional layers
conv1, weights_conv1 = new_conv_layer(input=x_img,
                                      num_input_channels=num_channels,
                                      filter_size=filter_size1,
                                      num_filters=num_filters1,
                                      use_pooling=True)
print(conv1)

conv2, weights_conv2 = new_conv_layer(input=conv1,
                                      num_input_channels=num_filters1,
                                      filter_size=filter_size2,
                                      num_filters=num_filters2,
                                      use_pooling=True)
print(conv2)

# The convolutional layer above need to be connected to the fulluy connected
# layer but the output of layer 2 must be reshaped
layer_flat, num_features = flatten_layer(conv2)
print(layer_flat, num_features)

# Create the first fully connected layer
fc1 = new_fc_layer(input=layer_flat,
                   num_inputs=num_features,
                   num_outputs=fc_size,
                   use_relu=True)
print(fc1)

# Create the second fully connected layer
# The output of this one gives the 10 classes yet to be "normalized" (softmax)
fc2 = new_fc_layer(input=fc1,
                   num_inputs=fc_size,
                   num_outputs=num_cls,
                   use_relu=False)
print(fc2)
dropout = tf.layers.dropout(inputs=fc2, rate=0.4)
# Apply softmax to the output of the fully-connected layer such that the output
# is in form of probability
y_pred = tf.nn.softmax(fc2)
# Transform the prediction from one_hot encoded to class form
y_pred_cls = tf.argmax(y_pred, dimension=1)

# Create the cost function to be optimized
cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc2, labels=y_true)
cost = tf.reduce_mean(cross_entropy)
# Create the optimizer in order to calculate the right parameter for the NN
optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)

# Calculate the accuracy of the algorithm
correct_prediction = tf.equal(y_pred_cls, y_true_cls)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

saver = tf.train.Saver()
save_dir = '/Users/claudiopaliotta/Documents/MachineLearning/Tensorflow_CNN/checkpoints/'
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

save_path = os.path.join(save_dir, 'best_validation')


# --------------------------------------------------------------
# SESSION
session = tf.Session()
session.run(tf.global_variables_initializer())

# Definition of the train batch
train_batch_size = 64  # number of images to be fed at each iteratioin in the optimizer

best_validation_accuracy = 0.0
last_improvement = 0
required_improvement = 1000

total_iterations = 0

# Helper function to perform the optimiziation iterations
optimize(num_iterations=5000)
print_test_accuracy()

img_cld = [img1, img2]
img_cld = np.reshape(img_cld, [2, 784])
# x_batch_pred = np.reshape(img_cld, [1, img_size, img_size, num_channels])
hot_lbl = session.run(y_pred, feed_dict={x: img_cld})
hot_lbl_cls = session.run(y_pred_cls, feed_dict={x: img_cld})
plot_image_pred(img1, str(hot_lbl_cls[0]))
plot_image_pred(img2, str(hot_lbl_cls[1]))
